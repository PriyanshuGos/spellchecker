# -*- coding: utf-8 -*-
"""Untitled1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/11-RKbL6HOC_bYEhB_725DNCGTvXmKKCP
"""

import nltk
nltk.download('stopwords')





from nltk.stem import PorterStemmer
from nltk.corpus import stopwords
nltk.download('omw-1.4')
nltk.download('wordnet')
from nltk import word_tokenize 
from nltk.util import ngrams
nltk.download('punkt')
import numpy as np
from nltk.tokenize import word_tokenize
import re
import string
from nltk.stem import WordNetLemmatizer
!pip install pattern
from pattern.text.en import singularize
from itertools import islice

import pkg_resources
!python -m pip install -U symspellpy
from symspellpy import SymSpell
import pandas as pd
from symspellpy import Verbosity
import csv

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

from wordcloud import WordCloud




filename='/content/sample_prod_names_tiles_sarees.csv'

df = pd.read_csv(filename)

series= " ".join(df["product_name"])

series

unigrams = series.split(" ")

unigrams

"""bigrams = []
for i in range( len(unigrams )-1) :
  bigrams.append( unigrams[i] + " "+ unigrams[i+1].lower())

bigrams

freq = (pd.Series(unigrams).value_counts() )

freq

freq1 = (pd.Series(bigrams).value_counts())

freq1

fig ,ax = plt.subplots(1,1,figsize = ( 16 , 8 ) )
freq = (pd.Series(unigrams).value_counts()[:20] )
sns.barplot( freq.index , freq)
_ = plt.xticks(rotation = 90)

'''fig ,ax = plt.subplots(1,1,figsize = ( 16 , 8 ) )
freq = (pd.Series(bigrams).value_counts()[:20] )
sns.barplot( freq.index , freq)
_ = plt.xticks(rotation = 90)"""

len(series)



### For BiGram
bi_gram = list(ngrams(word_tokenize(series), 2)) 
print(bi_gram[:10])

stemming = PorterStemmer()
wnl = WordNetLemmatizer()
stop_words = stopwords.words("english")
punc = string.punctuation
#spec_chars = ["null","nan","\\n","\\xa0","!",'"',"#","%","&","'","(",")",
 #             "*","+",",","-",".","/",":",";","<",
  #            "=",">","?","@","[","\\","]","^","_",
   #           "`","â","€","™","{","|","}","~","–","0","1","2","3","4","5","6","7","8","9"] 
print(stop_words)

def data_clean(a):
    
    b = a.lower()
    #print("done")
    #c = [d for d in b if d not in punc]
    #c = ' '.join(c)
    b = re.sub('[^a-zA-z]', ' ', b)
    
    #c = b.split()
    #d = " ".join([singularize(str(d)) if len(str(d)) > 2 and str(d) not in stop_words and not (str(d)).endswith('i') else d  for d in c])
    clean_text = []
    for word in b.split():
        if word.endswith('i') and len(word) > 2 and word not in stop_words:
            clean_text.append(word)
        elif len(word) > 2 and word not in stop_words:
            clean_text.append(singularize(word))
    return " ".join(clean_text)
    #d = d.replace('sare','saree')
    #print(c.shape)
    #c = c.iloc[0]
    #print(c.iloc[0])
    #return [d for d in c if str(d) not in stop_words]
    return d

filename='/content/sample_prod_names_tiles_sarees.csv'
df = pd.read_csv(filename)
df.head()



df['clean_name'] = df['product_name'].apply(lambda x:data_clean(x))
df.head()

for i in range(20) :
  if(df['product_name'][i]!=df['clean_name'][i]) :
    print (df['product_name'][i])
    print (df['clean_name'][i])

### For UniGram
uni_gram = []
for line in df['clean_name'].values:
  uni_gram.append(line.split())
print(uni_gram[:10])

### For BiGram
bi_gram = []
for line in df['clean_name'].values:
  bi_gram.append([" ".join(e) for e in ngrams(line.split(), 2)])
print(bi_gram[:10])

bi_gram = np.concatenate(bi_gram)
bi_gram

uni_gram = np.concatenate(uni_gram)
uni_gram

freq_bi_gram = dict(pd.Series(bi_gram).value_counts())
#freq_bi_gram

freq_uni_gram = dict(pd.Series(uni_gram).value_counts())
#freq_uni_gram
print(len(freq_uni_gram))

print(freq_uni_gram['kamila'])

wrong_words=[]

reference_uni_gram = {}
for key,val in freq_uni_gram.items():
  
  if val > 50:
    reference_uni_gram[key] = val
  else:
    wrong_words.append(key)
print(wrong_words)

reference_bi_gram = {}
for key,val in freq_bi_gram.items():
  if val > 50:
    reference_bi_gram[key] = val

#reference_uni_gram
uni_gram_ref_file = open('uni_gram_ref_file.csv', 'wt')
uni_gram_ref_file.write(str(reference_uni_gram))
uni_gram_ref_file.close()

#reference_bi_gram
bi_gram_ref_file = open('bi_gram_ref_file.csv', 'wt')
bi_gram_ref_file.write(str(reference_bi_gram))
bi_gram_ref_file.close()

fig ,ax = plt.subplots(1,1,figsize = ( 16 , 8 ) )
freq = (pd.Series(uni_gram).value_counts()[:20] )
sns.barplot(x= freq.index , y=freq)
_ = plt.xticks(rotation = 90)



with open('test.csv', 'w') as f:
    for key in reference_uni_gram.keys():
        f.write("%s,%s\n"%(key,reference_uni_gram[key]))
file = pd.read_csv("test.csv")
headerList = ['word', 'frequecy']
file.to_csv("test.csv", header=headerList, index='false')
csv_file = 'test.csv'

# specify the data for the new row
new_row = [0,'saree', 10000]

# open the CSV file in append mode and write the new row
with open(csv_file, 'a', newline='') as file:
    writer = csv.writer(file)
    writer.writerow(new_row)
kd = pd.read_csv('test.csv')
kd.head()

sym_spell = SymSpell()

sym_spell.load_dictionary('/content/test.csv', 1, 2, separator = ",")

print(sym_spell.words)

previous_name = list(df["clean_name"])
tef = 30;
for i in range(len(previous_name)):
  y = previous_name[i]
  y = y.lower()
  suggestions = sym_spell.lookup_compound(y, max_edit_distance=2)
 
  for suggestion in suggestions:
   
    x = suggestion._term
    if(y!=x):
      print(y)
      print(x)
      tef -= 1
    if(tef <=0):
      break
  if(tef <=0):
      break



input_term = "sadi"
suggestions = sym_spell.lookup(input_term, Verbosity(2)  , max_edit_distance=2)
# display suggestion term, edit distance, and term frequency
for suggestion in suggestions:
    print(suggestion)

# now every word is checked with symspell

def symspello(c):
  suggestions = sym_spell.lookup(c, Verbosity(2)  , max_edit_distance=2)
  # this is the dictionary provided to soundex
  Soundex_dictionary = {}
  Edit_dist ={}
  for suggestion in suggestions:
    x = suggestion._term
    y = suggestion._count
    z = suggestion._distance
    
    Soundex_dictionary[x]= y
    Edit_dist[x]= z
  return Soundex_dictionary,Edit_dist

# Soundex fuction
!pip install libindic-soundex
!pip install libindic.utils
from libindic.soundex import Soundex
instance = Soundex()

def correction(word, dictionary):
    code = instance.soundex(word)
    candidates = [entry for entry in dictionary if instance.soundex(entry) == code]
    
    if (len(candidates)==0):
     return word
    new_word = min(candidates, key=lambda candidate: levenshtein_distance(candidate, word))
    if(freq_uni_gram[new_word]>(4*freq_uni_gram[word])):
      return new_word
    return word

def levenshtein_distance(s1, s2):
    if len(s1) < len(s2):
        s1, s2 = s2, s1
    distances = range(len(s1) + 1)
    for index2, char2 in enumerate(s2):
        newDistances = [index2 + 1]
        for index1, char1 in enumerate(s1):
            if char1 == char2:
                newDistances.append(distances[index1])
            else:
                newDistances.append(1 + min((distances[index1], distances[index1 + 1], newDistances[-1])))
        distances = newDistances
    return distances[-1]

def spellchecker(a):
  words = a.split()
  
  '''for evr in words:
    dict1, dict2 = symspello(evr)
    act_word = correction(evr,dict1)'''
  b =" ".join(correction(evr,symspello(evr)[0]) for evr in words)
 
  return b

df['new_name']= df['clean_name'].apply(lambda x:spellchecker(x))

df.head()

res= 100
i =0
df['clean_name'] = [sentence.replace('.', '') for sentence in df['clean_name']]
while (res>0):
  if(df['clean_name'][i]!=df['new_name'][i]):
  
    print(df['clean_name'][i])
    print (df['new_name'][i])

    res=res-1
  i=i+1

import tensorflow as tf
tf.__version__
print(df['clean_name'])

from tensorflow.keras.layers import Embedding
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.models import Sequential
from tensorflow.keras.preprocessing.text import one_hot
from tensorflow.keras.layers import LSTM
from tensorflow.keras.layers import Dense
from tensorflow.keras.layers import Bidirectional
from tensorflow.keras.layers import Dropout

voc_size = 50000
corpus = df['clean_name']

onehot_repr=[one_hot(words,voc_size)for words in corpus] 
onehot_repr

freq_uni_gram['foot']

sent_length=10
embedded_docs=pad_sequences(onehot_repr,padding='pre',maxlen=sent_length)

print(embedded_docs)

## Creating model

words_2d_list = [sentence.split() for sentence in corpus]

print(corpus[1])

import gensim
from gensim.models import Word2Vec
model = Word2Vec(words_2d_list, min_count=1)
model.wv.save_word2vec_format('my_word2vec.bin', binary=True)
v1 = model.wv['saree']
print(v1)

import numpy as np
import tensorflow as tf
import gensim

from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Bidirectional, Dense, Embedding
from sklearn.metrics.pairwise import cosine_similarity
from gensim.models import KeyedVectors
word_vectors = KeyedVectors.load_word2vec_format('/content/my_word2vec.bin',binary=True)


model = tf.keras.Sequential([
    tf.keras.layers.Embedding(len(word_vectors.key_to_index), 100, input_length=15),
    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64, return_sequences=True)),
    tf.keras.layers.Dropout(0.5),
    tf.keras.layers.TimeDistributed(tf.keras.layers.Dense(32))
])

# Compile the model
model.compile(loss='mse', optimizer='adam')

# Convert each sentence in the list of sentences into a sequence of word indices
max_seq_length = 15
word_to_index = {word: index for index, word in enumerate(word_vectors.key_to_index)}
sequences = []
for sentence in corpus:
    words = sentence.lower().split()[:max_seq_length]
    sequence = [word_to_index.get(word, 0) for word in words]  # Use 0 for out-of-vocabulary words
    sequence += [0] * (max_seq_length - len(sequence))  # Pad the sequence with zeros
    sequences.append(sequence)

# Encode each sentence into a sequence of encoded vectors
sequences = np.array(sequences)
encoded_sequences = model.predict(sequences)

"""for i, sentence in enumerate(corpus):
    words = sentence.lower().split()
    for j, word in enumerate(words):
        if (freq_uni_gram[word]<50):
          if word in word_vectors.key_to_index:
            word_index = word_to_index[word]
            word_vector = word_vectors[word]
            cosine_similarities = np.dot(encoded_sequences[i, j], encoded_sequences[i].T) / (np.linalg.norm(encoded_sequences[i, j]) * np.linalg.norm(encoded_sequences[i], axis=1))
            most_similar_index = np.argmax(cosine_similarities)
            most_similar_word = list(word_to_index.keys())[list(word_to_index.values()).index(most_similar_index)]
            print(f"Original word: {word}, most similar word: {most_similar_word}")"""

input_term = "sadi"
suggestions = sym_spell.lookup(input_term, Verbosity(2)  , max_edit_distance=2)
for suggestion in suggestions:
  word1_vec = word_vectors['sadi']
  word2_vec = word_vectors[suggestion._term]

# calculate cosine similarity
  similarity = cosine_similarity([word1_vec], [word2_vec])[0][0]

  print(f"The cosine similarity between 'sadi' and {suggestion._term} is {similarity:.2f}")